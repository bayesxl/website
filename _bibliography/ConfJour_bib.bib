@inproceedings{cohen2020pessimism,
    title = {Pessimism About Unknown Unknowns Inspires Conservatism},
    url = {https://pdfs.semanticscholar.org/cece/bc0c325a9fc58e78d82a42c8b3f2d9bce769.pdf},
    abstract = {If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent's pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent's model class, a sufficiently pessimistic agent does not cause ``unprecedented events'' with probability $1-\delta$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent's policy's value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.},
    language = {en},
    booktitle = {Conference on Learning Theory},
    author = {Cohen, Michael K and Hutter, Marcus},
    year = {2020},
    pages = {1--30},
    file = {cohen2020pessimism.pdf}
}


@inproceedings{nguyen2020knowing,
    title = {Knowing The What But Not The Where in Bayesian Optimization},
    url = {},
    abstract = {Bayesian optimization has demonstrated impressive success in finding the optimum input x and
output f* = f(x*) = max f(x) of a black-box function f. In some applications, however, the optimum output f*
is known in advance and the goal is to find the corresponding optimum input
x*. In this paper, we consider a new setting in BO in which the knowledge of the optimum output f is available. Our goal is to exploit the
knowledge about f* to search for the input x* efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information
about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization. We show that our approaches work intuitively and give quantitatively better performance against
standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.},
    language = {en},
    urldate = {},
    booktitle = {International Conference on Machine Learning},
    author = {Nguyen, Vu and Osborne, Michael},
    year = {2020},
    note = {Link to github code, if there is code},
    pages = {},
    file = {nguyen2020knowing.pdf}
}


@InProceedings{wagstaff2019limitations,
  title = 	 {On the Limitations of Representing Functions on Sets},
  author = 	 {Wagstaff, Edward and Fuchs, Fabian and Engelcke, Martin and Posner, Ingmar and Osborne, Michael A.},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6487--6494},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {Wagstaff2019Limitations.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wagstaff19a.html},
  abstract = 	 {Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimension of this latent space may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this is only of limited practical use. Motivated by this observation, we prove that an implementation of this model via continuous mappings (as provided by e.g. neural networks or Gaussian processes) actually imposes a constraint on the dimensionality of the latent space. Practical universal function representation for set inputs can only be achieved with a latent dimension at least the size of the maximum number of input elements.}
}


@inproceedings{ru2020bayesian,
    title = {Bayesian optimisation over multiple continuous and categorical inputs},
    url = {},
    abstract = {Efficient optimisation of black-box problems that comprise both continuous and categorical inputs is important, yet poses significant challenges. Current approaches, like one-hot encoding, severely increase the dimension of the search space, while separate modelling of category-specific data is sample-inefficient. Both frameworks are not scalable to practical applications involving multiple categorical variables, each with multiple possible values. We propose a new approach, Continuous and Categorical Bayesian Optimisation (CoCaBO), which combines the strengths of multi-armed bandits and Bayesian optimisation to select values for both categorical and continuous inputs. We model this mixed-type space using a Gaussian Process kernel, designed to allow sharing of information across multiple categorical variables; this allows CoCaBO to leverage all available data efficiently. We extend our method to the batch setting and propose an efficient selection procedure that dynamically balances exploration and exploitation whilst encouraging batch diversity. We demonstrate empirically that our method outperforms existing approaches on both synthetic and real-world optimisation tasks with continuous and categorical inputs.},
    language = {en},
    urldate = {},
    booktitle = {International Conference on Machine Learning},
    author = {Binxin, Ru and Alvi, Ahsan and Nguyen, Vu and Osborne, Michael and Roberts, Stephen},
    year = {2020},
    note = {https://github.com/rubinxin/CoCaBO_code},
    pages = {},
    file = {ru2020bayesian.pdf}
}
